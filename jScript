pipeline {
    agent any

    environment {
        // Databricks cluster ID and notebook path
        CLUSTER_ID = '2236063f6f28e164'  // <-- replace with your actual cluster ID
        NOTEBOOK_PATH = '/Workspace/Users/hritik.moon@incredotech.com/SStream/SparkStreaming/API_stream'
        DATABRICKS_TOKEN = credentials('DATABRICKS_TOKEN_2') // Jenkins credential ID
    }

    stages {
        stage('Checkout SCM') {
            steps {
                checkout([$class: 'GitSCM',
                          branches: [[name: '*/main']],
                          userRemoteConfigs: [[url: 'https://github.com/Hritik-Incredo/SStream.git']]])
            }
        }

        stage('Install Databricks CLI') {
            steps {
                bat 'python -m pip install --upgrade pip'
                bat 'pip install --upgrade databricks-cli'
            }
        }

        stage('Prepare Notebook JSON') {
            steps {
                script {
                    // Create JSON for Databricks run
                    def jsonContent = """
{
  "run_name": "Jenkins Notebook Run - Build #${env.BUILD_NUMBER}",
  "existing_cluster_id": "${env.CLUSTER_ID}",
  "notebook_task": {
    "notebook_path": "${env.NOTEBOOK_PATH}"
  }
}
"""
                    writeFile file: 'notebook_run.json', text: jsonContent.trim()

                    // Debug: print JSON content
                    bat 'type notebook_run.json'
                }
            }
        }

        stage('Run Databricks Notebook') {
            steps {
                // Run notebook and fail Jenkins if it fails
                bat """
                    databricks runs submit --json "@notebook_run.json" --wait
                """
            }
        }
    }

    post {
        success {
            echo "Databricks notebook completed successfully."
        }
        failure {
            echo "Databricks notebook failed. Check logs!"
        }
    }
}
